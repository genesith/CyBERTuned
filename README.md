# CyBERTuned

CyBERTuned is a BERT-like model trained with an NLE (non-linguistic element) aware pretraining method tuned for the cybersecurity domain. The paper can be found [here](https://aclanthology.org/2024.findings-naacl.3/) and the model weights can be found on [Hugging Face](https://huggingface.co/s2w-ai/CyBERTuned-SecurityLLM).


# Citation
If you're using CyBERTuned please cite the following paper:

```
Eugene Jang, Jian Cui, Dayeon Yim, Youngjin Jin, Jin-Woo Chung, Seungwon Shin, and Yongjae Lee. 2024. Ignore Me But Don’t Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 29–42, Mexico City, Mexico. Association for Computational Linguistics.
```
